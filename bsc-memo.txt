"Software Security as a Financial and Operations Research Problem"
Jukka Paulin
Aalto University
Department of Computer Science

2017-2018
B.Sc. thesis memo

Introduction: Why Software Security Matters?

We're entering a world where software makes a great deal of
our daily lives. Software is a critical part of both "actually
useful" as well as the trivial and fun stuff in our lives.
Nevertheless, in both cases we'd like to be assured of certain
things: that a life-saving thing like the brakes of our car,
or the control unit of an elevator, or the software used by
air traffic controllers (ATC personnel)

Then again, it doesn't excuse bad behavior either, even if
we are talking about a little furry children's toy that
has a microphone and an Internet connection built-in. We certainly
don't expect nor allow that kind of a toy to turn into
a spying ear in our homes.

Software developers ('devs') are at the heart of this process.
They are hired by corporations to do work that ranges from
A to Z; often times developers themselves might not be
the core subject matter experts, ie. a developer working on
a software that ultimately is _used by_ a person working as
Air Traffic Controller

So there's a fundamental separation of concerns; an in reality,
there are even more, hierarchical separations of concerns:
the corporation producing this ATC software most likely has
personnel in management positions, who might not do the
actual development (coding) work at all.

The separation of concerns requires coordination, so that
the software gets done properly: that it fits the particular
purpose (the very reason it is being written); that it
does not do things which are bad (security issues); and
to count for the case of real world, the corporation that
does the software must most likely also maintain its processes
viable cost-wise: otherwise, one day the corporation finds
out it has lost the deal to a competitor.  

---

Problem-solving strategy for this Bachelor's thesis:

The First Question:

 "Can secure programming be fun and adapted as a new style?"

Does Software security really have to feel a "drudge", like making
your bed every morning? Or can it actually be a natural improvement
in the software methodology, so to produce more secure software without
extra effort, once the developers have accustomed to the style?

Second Question:

 "Can the good-quality programming be adopted team-wide and be
  rather static, without constant rat race with vulnerabilities?"

Finding the relations between types of vulnerabilities and
the amendments in software development process (SDP). We assume that
change always incurs some costs, and since the paper is about exploring
"minimizing costs while maintaining or improving quality" this is
a very central question.


Project models in Software engineering

* introducing what a SW development project model is
* process can deviate from a model
* metrics can be introduced to track deviation and development progress
* how can feedback *guide* the (real) process towards optimum?
* should project model change during time?

 -> gate (checkpoint)

'Hacking' is something that doesn't particularly care about
or need a project model. There are numerous tales (often admiring
in tone) of hackers who can pour thousands of lines of software,
laying down major chunks of the project at one go; however, what
might be untold in these tales is the long-term effects of the
heroic deeds: other participants in the project might have tough
time adjusting to the change, and if the chief (often called
"wizard" or "guru") isn't too keen on documenting what he does,
the team might be completely lost.

A project model means a template of the instructions that
would be given to a software team, in order to make progress,
and produce good quality software.

Project models have evolved throughout the (relatively short)
history of software.

To recap the highlights of the project models:

pre-1950s: no models at all?
[WIP]
2005-2017: Agile is all the rage

[WIP: Selvitä termiviidakko ohjelmistokehitysmenetelmistä]
Scrum 
DSDM
Extreme Programming (XP)
Crystal Methods
Adaptive software development
Pragmatic Programming
Feature driven development
Gilb-EVO


------------------------------------------------------------

Defining metrics, processes, and variables

* how to measure change from altering the variables
* no direct, absolute "S->R" but more likely a gradual
  response, See: https://en.wikipedia.org/wiki/Stimulus%E2%80%93response_model

To have researchable results, we must have some control over
the process ('variables'). The variables are things that can be
altered (my management). Teams respond to new rules probably
in somewhat unpredictable way; this is the same as in economics:
the response of a group of people cannot be foretold exactly.

For the research method it's quite important to think, how
the response (shown in metrics) is regarded. Let's say there's
a change in variable 'Sprint length'. This variable is easy,
since the team cannot arbitraly choose not to obey the change.

What about a technical measure, like 'Maximum allowed defect
count'? Again, the process adherence defines how the team
reacts. If it's a hard limit, in a way that simply the team
is not allowed to proceed until the bugs have been fixed to lower
count, it's easier for our research to relate to this. If
it's a soft threshold and team can continue doing code as
if there wasn't a maximum amount on defects, it's harder to
establish what the real change would be from such change.

Variable list

Sprint length
Maximum allowed defect count
Technical Debt limits

------------------------------------------------------------

- no project model ("ad hoc")
- waterfall
- iterative (aka incremental model)
- see https://en.wikipedia.org/wiki/Waterfall_model 

Some of the apparent "change" in methodologies might be 



[WIP] More rigorous limits and dissecting what the different
      models were, when have they emerged, are they still
      in use, how one can identify being a user, and
      pros/cons of each

Characteristics (parameters) of software projects

scope
costs
phases
timetable

Time-specific items of the Characteristics

first version ship date
"1.0" version ship (first functional version of SW)

Phases of software engineering

conception
initiation
analysis
design
construction
testing
production
maintenance
decommissioning

------



Change

Software, by its very nature, is evolving. One of the pitfalls of
early "waterfall" project management models was that Teams spent
enormous time and effort in defining software up-front, without
making prototypes. This was later countered by the YAGNI in Agile
methodologies ("You ain't gonna need it")
See [5]


Process parts in Software Development
- the Agile model
  - cycles

A process is often defined for software development in order
to mitigate the negative effects of "having to re-invent a wheel".
Agile (the Manifesto) is one of the most talked-about and
currently probably most widely used software development methodology.
Agile development is a people-first attitude and a counterforce
to the "tar-pit" effect, which was observed by 



=========================================================================
Major Building steps for paper
=========================================================================
- pose the central questions as topic says: "Operations Research" problem
  > so introduce all that is promised
  > implement both a Financial and OR viewpoint
- quantifying a SW project
  - by the process itself
  - documenting the goals of a software project
  - tracking in time (usually done with a Agile tool)
    - daily sampling?
    - enough log of code repository (ie. complete 'git' history available)
      to do postmortem analyses
      
- add empirical testing to disprove or prove the hypotheses

- set up multiple, close-enough software projects that can
  differentiate the results
- set up the framework for measurement, similar in all projects

- consider technical limitations to the lab;

- A practical problem:
   how can there be several Software projects whose nature is similar,
   but the results are different?


Tracking items for writing process
==================================

[WIP:Figure]	add a figure (graph, image)



============
Introduction
============

The Purpose of software: Utility

Software Engineering produces computer code and other artefacts, which
are executed by a Computer. This ('software') can be either visible
or invisible part of a commodity that has an utility for its Users.

Depending on whether a project is commercial (tailored software) or
a open source project, there can be quite different stakeholder
groups and even a very different philosophy in design and practical
issues. Open source software is produced "by the people, for the people"
and often solves a common problem. Shrink-wrapped, commercial projects
are done often for a single customer, who is willing to significantly
put resources behind the development. In commercial projects the


Characteristics of Commercial vs. FLOSS software

Both:
Span
Scope
Contract

Both:
Changing team members (possibly)
Documentation
Code
Artefacts
Supporting infrastructure for builds

- can software be "scrapped" ? Ie. abandoned
- liability issues
- contract breach
- pricing


Software Drift: "It was never supposed to be there"

Security breaches are enabled by many things, but one of the
factor that creeps from human assumptions is that the software
team responsible for developing the code might never imagine
the software to be used in a particular setting.

Especially if one looks at the open source community, many
projects are described as "just a little ..." this-and-that. So
there are humble beginnings, and often the software might
be custom built just to serve a transient need. Or so it is
thought - because we are lazy as a community, and programmers
especially believe the mantra of DRY (Don't Repeat Yourself),
it's a natural inclination to use existing solutions as long
as they fit the gap. 

"Small web servers" then one day find their way not only
to rather closed and protected intranets, but indeed are out
in the wild, facing all the hostile network traffic the world
of Internet can bring about. These same software are also running
perhaps as a utility glue in children's toys and what-not -
essentially in as many places as you could NOT imagine.
People might have forgotten (during a typical lifespan of 5-10
years) the origins of the software, and might take for granted
certain security properties - "since it (software) is popular
and has been time-tested".

For example, some small utility programs nowadays are found as
embedded components in many of the so-called "IoT" devices. A piece
of software that originally was supposed to be almost a passing
"patch", to satisfy a particular need, often keeps persisting,
gathering more code, and the software "drifts" to random places.

This very factor is part of the Open Source movement's power:
software is omnipresent and easily available, without limitations
placed upon its use. The same philosophy implies a kind of
'caveat emptor' (know what you are doing, as a developer).


Chapter ?X: Why programming is mentally hard?

A simple recipe, from anyone who has studied computer science, is
that 'data structures + algorithms = code'. So in theory it would
be quite simple to do programming. Since algorithms have become
very abundant, being development in laboratories, universities, and
often also being published, we'd intuitively think that software
would be even easier than placing LEGO blocks on top of each
other.

The problem emerges from these: 
 - there are gray areas, not clear-cut cases

"Gray area" ?

The gray areas have to do with ambiguity of drawing a line in classifications,
for example. Let's say I want to 'detect operating system'. We have 3
OS available: Windows, MAC, and Linux. It is superficially an easy thing to
do. There's an environment variable (a string) available and it is set to
contain information that is input to our function.

So return value is one of 4 possible values: three choices, and the error
case ('unknown') where we couldn't determine the OS.

We have to communicate this properly. It would be very "nice" to always
fall into one OS, since otherwise the downstream developer will have
to make the decision on the gray area: how to proceed with a 'unknown'.

Tallying up the uncertainty of Gray areas
- leads to combinatorial amount of branches
- coverage comes to play
- also a question of understanding, communication
- but remember: hackers are only looking for holes, not explanations
  - management might be interested in explanations
- so called "water tight code"

---

IoT era paradox: imbalanced whole
- increasing prevalence; devices abound
- manufacture is cheap
- as devices are trying to be kept minimal in price, software development
  costs are also being curbed
  -> insecurity
- a kind of imbalance between the nature of hardware vs. software
  - "once right" hardware design can be replicated with low costs
  - software often needs more tending, as a modern-size software project
    can never be "proven correct"
- devices coming into more "physical" and real danger -imposing
  use cases


Technical principle of Computers and Software: Short intro
----------------------------------------------------------
Software is comprised of code. Code is language, that describes
(ultimately) arithmetic operations that the computer will
execute. Computer can manipulate 'bits': smallest digital units
of data. These bits can be used to represent meaningful
concepts in the Domain of the software.

Computer code is representation of the actions that the CPU and
other processors shall execute, in combination with memories (RAM)
and static storage (hard disk, other media). The code directs
computing.
[See: Von Neumann architecture]

Metrics of Software Quality
---------------------------
* [WIP] The distinction of quality metrics in Static source code
  vs. the overall (security) functional quality of a program
  when it is being run (live)

The quality (goodness) of a software has many metrics. One of them
is the security:

  a) can the software be fooled to do things it
     wasn't supposed to, and
     
  b) can a malicious user "break" the software and use to it go
     through security perimeters that were supposed to keep him out?

Security is often overlooked in production

Security is one of the hardest parts of software. It is considered
a kind of "secondary" aspect, since security by itself does not
directly contribute to the perceived value a Software has. Software
was originally written to solve a problem, or to provide automation
and service. Thus security comes in as a side effect; it can be
described as the amount of unintended malicious (harmful) use
cases that can be executed.

What is Secure programming?

- algorithm quality
  - random number generation
- data disclosure
  - session restart / force vulnerabilities
  - if you reuse (even accidentally) data in security protocols,
    you most likely give attacker a big hand
- "beyond your abstraction": looking at the computer, not just
  JavaScript or Java and JVM
  - some things get broken due to the nature of computer as
    something that acts in shady ways, compared to what you as
    programmer think

"Computers don't really know what they are doing"

Computer, and software, lacks "sentient introspection": it doesn't
know when it is being fooled. If a randomness algorithm relies
on a division operation, and the divisor is the time difference
between two events (supposed to be some kind of "random" number
between 1 - 1,000,000,000 since we measure nanoseconds), it will
be a major disruptor if an attacker can "drive" the process so that
the difference is always 1 ns. Thus the division is X/1 which is
always X itself: that might be a leak of data.

We humans often have "wit" and general intelligence, which can
defend us against certain class of evident attacks, but computers
work just as they are told to. Thus the intelligence of a system
(software) is as good as the wits of the developers. Note plural:
not just "you", but all developers that have done code that you
rely on; it might be libraries, kernel, core BIOS, etc.


Chapter ?X: Software Testing

Definition of "correct" vs. good-enough 
- All software will have bugs
  - only very simple pieces of code can be VERIFIED to be "correct"
- Finding sweet spots in methods
  - not a formality?
  - efficiency of writing minimal but sufficient amount of tests
    goes up with programmer's experience in the subject
- Methods of testing
  - black box
  - gray box
  - white box testing
- Do the "vulnerability seriousness" rankings follow some kind of pattern / relation
   to the effort in SW engineering ?
  - ie. can we predict where security should be increased in a Software project?

1) Logic level testing
2) Real: Stack frames, overruns etc. accounted for

The Ishikawa (fish-bone) diagram for Root Cause Analysis


Sub-Chapter: Nature of Discovering Bugs

Discovering bugs can happen basically in various ways:

  accidentally,
  by intent,
  during Quality Assurance;
  and coming as feedback
  from the wild (a vulnerability disclosure).

The important thing about bug discovery is "When" and "Where". Financially
it makes a big difference, whether bug is discovered inside or outside
the software house. Outside discoveries often have the potential of
causing direct or indirect losses for the company and its clients.

 - a developer reading source code, haphazardly, and stumbling upon a bug
   (piece of source code that looks suspicious)
 - writing unit tests and running them -> show a bug
 - bug from "the wild": there's a disclosure of vulnerability
 - a specific test of Quality Assurance called "penetration testing" 

Even a developer who has written the code ("author") may have trouble
deciding upon the consequences of a "fix" - editing the source code
to a different state. Unit tests may guard against problematic changes;
see [Unit tests: Coverage]

Undoing benevolent source code change by accident

Other developers may not always understand changes. I was once working in
a project where a bug was looming due to the obscurity of JavaScript,
a language that has had a troubled past 



Chapter ?X: The nature of software vulnerabilities

Defining a vulnerability
Some central glossary
CVE identifiers


Chapter ?X: The Software Team - Human points of view

In reality, there are a lot of human-factor issues in software
engineering. Otherwise we'd probably see robots doing automated
code. People have motivation and incentives to act. An interesting
paper [2] I stumbled upon about two years prior to starting
writing this thesis, was one that introduces the roles of a Team
as viewed through mathematical "Game theory".

People have incentives to act or "be insensitive to matters";
they also have (according to paper's authors) a different
set of payoffs that direct the software development in a practically
skewed way: developers do not reap such a great extent of the fruit
of doing software, whereas product owners and sales (company reps)
are more likely to benefit from a successful software.

However the core of the problem is also the definition of
'successful software'. It is a lot about cost+feature issues on the
sales side, whereas developers are more inclined to tend for quality. 
Sales would like to push software out to clients fast, with a lot
of features, and with a competitive (preferably low, if there
is competition) price. However generally both # of features +
quality increases price, as there is more work to be done to
polish the software.

Examples of vulnerabilities
===========================

Class of security hazards: Does this project talk about viruses,
malware, or only about software defects?


Chapter ?X: Basics: Computing defined
=====================================

Computing is basically just retrieval of bits, manipulation of
the bits, and their storage back to memory. Thus described,
computing can be theoretically reverted to a 'Turing machine',
which has very simple operations on a infinite band.

In all practical sense, software is produced by people. Computer
programmers ('developers', software engineers) have the domain
knowledge and specialty to write code. In addition, a real Software
project often has many other roles as well: designers, a product
owner, etc. See: "Agile"


Software can be measured in two ways: static and dynamic.
 [WIP: Be more exact in 'measured' vs. 'analyzed'. Can software
 be "measured" in dynamic way?]

Static analysis is the software source code -level review, done
often by either people or people and tools. Tools can quickly count
the occurrence of various patterns, especially given the correct
level of inspection: typically free-text (variable and function
names chosen by developers) are transformed first systematically
to anonymous symbols, so as not to give focus on wrong things.

  It is important to remember that names and naming are one the
  key parts of program code. A name "affords" cognitive assumptions
  about the variable or function, thus proper and systematic
  use of naming is an important prerequisite to quality of code.

Examples: Naming convention in 'david' - a dependency tool

 'david' compares two sets of versions in software libraries:
  - those that are currently being used in your project vs.
  - the "available versions" of those particular libraries (on Internet)

  The tool helps a software developer judge whether he should be
  moving up to more recent versions of libraries in his project.

 Below are 10 function headers of real JavaScript code:

 function depType (opts) { ... };
 function getDependencies (manifest, opts, cb) { ... };
 function getLatestStable (versions) { ... };
 function getLatestVersionInfo (current, versions, opts) { ... };
 function getVersionsInfo (name, opts, cb) { ... };
 function getVersionsInRange (range, versions, loose) { ... };
 function isScm (version) { ... };
 function isStable (version) { ... };
 function isUpdated (dep, opts) { ... };
 function normaliseDeps (deps) { ... };

The verb 'is' implies that result will always be a Yes/No, thus a "boolean".
If another developer would later write functions to this code, it would
be highly scorned to have:

 function unstableStatus (version) { ... };

Instead, one should follow conventions and have:

 function isUnstable (version) { ... };

The conventions are often per-project, and they are formed somewhat based
on the "will" of the lead engineers at the beginning of the project.
There are variations in this area, and 

-----


Chapter ?X: Technical - Excluded in this thesis
===============================================

This thesis concetrates on processes, people, and technology that
directly contribute to software. Whereas the reality is that
all software needs to be run on a "hardware", for reasons of
scope the hardware considerations are ruled out. Thus all computing
happens on a kind of reference architecture, somewhat described
by the "von Neumann" chapter. 

Computing hardware and its role in security
- what is the class of attack, that exploits physical hardware properties
  - couple of examples
  - SSH (secure shell) RAM memory being swapped to unsafe places
    without first overwriting it with zeroes
    - SSH Communications patched the code later
  - similar bug with SSH clone, "Putty" (software)
    "PuTTY vulnerability private-key-not-wiped-2" (in Putty 0.63, fix in 0.64)
    [URL http://www.chiark.greenend.org.uk/~sgtatham/putty/wishlist/private-key-not-wiped-2.html]


Unit tests: Coverage
 - two kinds of coverage

 - understanding the coverage and details of what the term itself
   means (and whether referring to "line coverage" or "branch coverage")
   are important
 - a SW team might have a separate QA engineer and/or QA team for quality
 - writing unit tests often falls to the developers themselves
 


Tools of the trade: What developers use during Software project?
================================================================
Compilers
also "transpilers"

Build chains
Automation
Scripts
Linux/Windows/Mac

Technical solutions: disk space, etc.
Backup systems
'git' and other repository technologies (storing source code)
Sketching tools for rapid prototypes
Documentation and Tickets (quality assurance)
Time tracking tools

Environments (IDE) for doing code
Communications: team collaboration tools
Statistics and metrics

- tools have diversified
- what is the trend of teams, in terms of # of tools used?
  increasing, staying same, or decreasing?
  - does simplicity have a positive effect on project metrics?
    - and does a specific tool have short payback period in terms of investment?



Chapter ?x: Software project Tool payback investment calculations
Any tool implementation should be considered as Investment
- basic formula
  -
  



II Nature of sofware development

- paradigms during the ages
  Object oriented (Oo) 

All major paradigms of programming have kind of had
their "promise of an elegant solution" to the general
hardness of making consistently good, understandable
and maintainable code. In Oo, the promise goes along
this way: "Since software emulates real world, design
your code around Objects. These objects should mimic
the real world's objects, so that if you have a 'car'
in your software, make a data structure that has
the qualities of a real car, and then make functions
that operate on these qualities, and instantiate
Car Objects. The rest will follow."

Object oriented programming


Patterns

Software can be reduced to a "standard form" using AST. The
tree is particular to each programming language. Usually AST
is produced during a compilation (build). 

  Can AST be produced automatically from any given (binary)
  program?
 
  This product (the tree), is particular to a programming language.
  If, for example, one would obtain a binary executable program,
  without any knowledge of which language and compiler was used
  to produce it, a AST cannot be produced.
  
  Thus the reduction step can be carried out if there are
  2 things available: the complete source code of a program,
  and the language knowledge. 
  


(See: "Abstract Syntax Tree")
Instead of looking at the human-readable source code, when a program
is reduced to AST, the humane namings etc. are reduced to registers
(essentially an ordered set of memory locations with specific sizes).


Steps
=====

- define the scope of a Software project (should be quite formal)
  - best practises in industry?

- define the variables (metrics) of a SW project
- make functions that 


Research problem statement
==========================
'Can software security be improved while maintaining costs savings?'

 ie. Is there a process to keep quality high while not adding too much of
     overhead into programming?

=> If such a process exists, how can it be discovered? (Master's)
   + taught to a SW Team, and quality kept up (preventing fallback to
     "bad practises")

Hypotheses
==========
- H0: 'No can do.'
      quality and costs are interchangeable (ie. not possible to lower costs
      while keeping quality at same or higher level)
      
- H1: 'It is possible.'
      Software Quality can be increased while maintaining or lowering costs,
      by understanding that security issues are partly due to wrong methodology
      in SW engineering. Attack methodology as root cause of low quality.
      

How long is a proper time span?
- The hypotheses might be impossible to inspect in too short period of time
- it takes some time to make a proper sized SW project
- and check what its quality is in "real world"
  - # of post-live defects
  - # of incidents realized

Basics

Software introduction
Computer as a machine (the paradigm of von Neumann)
 https://en.wikipedia.org/wiki/Von_Neumann_architecture

Coalescence: Rise of open source software
 * CatB: The Cathedral and Bazaar (comparing two opposing views of software engineering)
 * technical infrastructure apt for the open source
 * benefits
 * drawbacks



This Thesis Paper should..
 - give advice as to whether certain New Methodology is better
   than old bag of knowledge
   - might advice on future research on change management
     to get the Management and Team follow the NM

Setup
- real or invented project?
- working from scratch or joining a already existing project?

 Scoping the state of project upon entry
  - is there a particular "problem state" or Showstopper, right now?
  - or a feeling of being "generally stuck" etc

III "von Neumann" computing platform

a von Neumann architecture

[WIP:Figure]


The Occam problem in selecting suitable libraries
=================================================
You're about to read or sing for a child in bed.
The situation requires quick actions; if you linger on,
your child gets frustrated. If you use wrong tools, you
will eradicate his need for sleep and he gets angry,
too. By selecting "just the right tool" (ie. a flashlight
with not too intense light) you are able to read
the story, without bringing too much light into the room.
However, as usual, mr. Murphy gets involved: "Where is that
flashlight?" -> searching costs. Judgment. Heuristics. Etc.

This story has a reminiscent in software engineering. Often
developers are aware of "existence of tools capable of solving
a practical problem", and intriguingly, nowadays the question
is more "What is the _right_ tool to do the task".

There are many ways to tackle such a selection problem;
some more "professional" or formal ways are to
- enumerate the choices
- evaluate the choices based on various methods and metrics
- choose a solution out of the candidate group
- possible be ready to yank the solution out and make another
  choice

Some Heuristics in "library selection problem"

Occam's razor is a philosophical tool that advises to choose
the simplest "thought" to suit a particular phenomena. In software,
Occam's principle might advise to choose the simplest solution.

However, contrary to this heuristic, programmers often think
grandiose: Slamming as many flies with one go as possible. Thus
the choice might be something else than the simplest.

Open-source and especially open registries of software have
exploded the market: ever smaller units (packages) are being
published, and thus creating at least a theoretical opportunity
for deeper scrutiny of the ingredients of software. Linus Torvalds
has said something that became the "Linus' Law":

  given enough eyeballs, all bugs are shallow.

===================================================================

Appendix 1: How come 'Heartbleed' was possible? 2 years in the dark

One of the suprising "news bombs" was the Heartbleed, a rather
minimal amount of flawed code, which nevertheless was positioned
in a critical place in the core of Internet security. The vulnerability
had the potential to endanger a vast amount of user passwords
and private information throughout millions of servers (PCs) connected
to the Internet. 

Heartbleed (officially CVE-2014-0160) is a security bug in OpenSSL
library, which is a widely used implementation of the
Transport Layer Security (TLS) protocol. Heartbleed was introduced
into the software in 2012, and publicly disclosed in April 2014.

The amount of server exploitation enabled by Heartbleed is unknown.
Heartbleed exploitation does not leave traces to logs, and thus the damage
quantity is hard to estimate.

-> How come Heartbleed could happen?
   - the lines of code #
   - remedy:
     - administrators: patch the server (change OpenSSL to
       non-vulnerable version)
       - make sure the vulnerability is not re-introduced by automatic
         "update" to OpenSSL
     - on user side: "Change your password"
       - if you change it before the server, that has Heartbleed,
         is patched then changing is not useful at all

Contrary:
 Heartbleed



Software Tools: Theoretical basis
=================================
To do effective measurement of various processes, each process must produce
metrics (Data) that is consistent and reliable. The production should be
economical; otherwise the scope of research is limited due to practical
issues with amount of hours put in to the research.


The basic model: A process
==========================

[WIP: Figure 3]
See the sample software process for Agile 2010-2016
- scan the page in


Several processes
==========================


How to compare the processes: Making 'ceteris paribus' real
===========================================================

In order to compare processes, it's vital to keep other things
similar; in latin, "ceteris paribus". This is the research
assumption that economics uses. Software engineering might have
similarities to economics, since we are talking about human
activity, time, resources, behavior, and incentives (and
disincentives aka. "penalty").

When I first thought of the thesis subject, I found that the
comparability might be a unsurmountable problem. I didn't
see any viable situation in real life (in software industry),
where I could keep other things intact (the team composition,
scope of project, subject of the project) while playing
god and altering just the process.







"The process" in our case is a Software engineering process:
[Figure] 

- quantifying the amount of pure in-house code vs. libraries
  - 'sloc' enhanced
- metrics ongoing
  - SLOC total in project
    - deltas per modules
    - sw modules can be classified into {tags}, or similar project management
      things
- incident count (known public vulnerabilities)
- occurrence
- Markovian
- stochastic simulation for bombardment of the software ("live" situation)

Tools: The setup of software for this thesis project
- "let me do my things": justification for automating most of the tools
- automation is king
- getting easy metrics

Quality metrics for this paper
- Quality defined properly using international IEEE standards
  - and any relevant Quality circle terminology

Nature of Security: Special attention required here
- pre-disclosure risk
- post-disclosure risk
  (Cadariu_2014)


Further research topics
=============================================================
1. Long-term viability of code
   - inspection of code change in a repository through years
   -> seeing patterns in what kind of code is stable
      - reasons might not be obvious at all
      - one possible explanation is "Too precious to remove",
        and/or "No one knows what exactly it does",
	"Too interconnected" (lot of downstream dependencies)
   -> social, meritocratic facets


References 

1. "Tracking known security vulnerabilities in third-party
   components"
   Master' thesis, Mircea Cadariu 2014
   local file: thesis-mircea.pdf

2. "Analyzing Software Development as a Noncooperative Game"
   https://www.cs.uic.edu/~drmark/index_htm_files/SDGames.pdf
   Mark Grechanik, Dewayne E. Perry
   ARISE (The Center for Advanced Research In Software Engineering)
   The University of Texas at Austin

3. "Cathedral and Bazaar"
   Eric Raymond

4. Domains (mathematical theory of programming)
   https://www.cs.rice.edu/~javaplt/311/Readings/domains.pdf

5. "Dreaming in Code" (book)
   Scott Rosenberg

Journal of my writing this thesis
Useful links:
 Editing text in Emacs efficiently
  1. http://ergoemacs.org/emacs/emacs_abbrev_mode.html
